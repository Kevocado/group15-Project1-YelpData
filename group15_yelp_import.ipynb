{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "import kagglehub\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "yelp_dataset = kagglehub.dataset_download('yelp-dataset/yelp-dataset')\n",
    "\n",
    "#load business data\n",
    "business_data = pd.read_json(f\"{yelp_dataset}/yelp_academic_dataset_business.json\", lines=True)\n",
    "\n",
    "#load reviews data in chunks\n",
    "reviews_chunks = pd.read_json(f\"{yelp_dataset}/yelp_academic_dataset_review.json\", lines=True, chunksize=10000) \n",
    "\n",
    "#store chunks in list of chunks\n",
    "reviews_df_list = []\n",
    "for chunk in reviews_chunks:\n",
    "    reviews_df_list.append(chunk)\n",
    "\n",
    "#combine to one df\n",
    "reviews_df = pd.concat(reviews_df_list, ignore_index=True)\n",
    "\n",
    "#load checkin data\n",
    "checkin_chunks = pd.read_json(f\"{yelp_dataset}/yelp_academic_dataset_checkin.json\", lines=True, chunksize=10000)\n",
    "checkin_df_list = []\n",
    "\n",
    "for chunk in checkin_chunks:\n",
    "    checkin_df_list.append(chunk)\n",
    "#combine to one df\n",
    "checkin_df = pd.concat(checkin_df_list, ignore_index=True)\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening Business into 3 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_attributes(row):\n",
    "    attributes = row.get('attributes', {})\n",
    "    if attributes:\n",
    "        attributes = json.loads(attributes) if isinstance(attributes, str) else attributes\n",
    "        flattened = []\n",
    "        for key, value in attributes.items():\n",
    "            if key == 'RestaurantsPriceRange2':\n",
    "                key = 'PriceRange'\n",
    "            elif key == 'RestaurantsDelivery':\n",
    "                key = 'Delivery'\n",
    "            elif key == 'RestaurantsTakeOut':\n",
    "                key = 'Takeout'\n",
    "            flattened.append({'business_id': row['business_id'], 'attribute_key': key, 'attribute_value': value})\n",
    "        return flattened\n",
    "    return []\n",
    "\n",
    "def flatten_hours(row):\n",
    "    hours = row.get('hours', {})\n",
    "    if hours:\n",
    "        hours = json.loads(hours) if isinstance(hours, str) else hours\n",
    "        flattened = []\n",
    "        for day, time_range in hours.items():\n",
    "            open_time, close_time = time_range.split('-')\n",
    "            flattened.append({'business_id': row['business_id'], 'day': day, 'open_time': open_time, 'close_time': close_time})\n",
    "        return flattened\n",
    "    return []\n",
    "\n",
    "# Flatten attributes and hours\n",
    "attributes_data = []\n",
    "hours_data = []\n",
    "\n",
    "for _, row in business_data.iterrows():\n",
    "    attributes_data.extend(flatten_attributes(row))\n",
    "    hours_data.extend(flatten_hours(row))\n",
    "\n",
    "# Convert to DataFrames\n",
    "attributes_df = pd.DataFrame(attributes_data)\n",
    "hours_df = pd.DataFrame(hours_data)\n",
    "\n",
    "# Drop the original nested fields from the business table\n",
    "business_flattened = business_data.drop(columns=['attributes', 'hours'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_business_table = \"\"\"\n",
    "CREATE TABLE business (\n",
    "    business_id VARCHAR(22) PRIMARY KEY,\n",
    "    name VARCHAR(255),\n",
    "    address VARCHAR(255),\n",
    "    city VARCHAR(100),\n",
    "    state VARCHAR(10),\n",
    "    postal_code VARCHAR(20),\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    stars FLOAT,\n",
    "    review_count INT,\n",
    "    is_open BIT,\n",
    "    categories TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_attributes_table = \"\"\"\n",
    "CREATE TABLE attributes (\n",
    "    attribute_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    business_id VARCHAR(22),\n",
    "    attribute_key VARCHAR(100),\n",
    "    attribute_value VARCHAR(MAX),\n",
    "    FOREIGN KEY (business_id) REFERENCES business(business_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_hours_table = \"\"\"\n",
    "CREATE TABLE hours (\n",
    "    hour_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    business_id VARCHAR(22),\n",
    "    day VARCHAR(20),\n",
    "    open_time VARCHAR(20),\n",
    "    close_time VARCHAR(20),\n",
    "    FOREIGN KEY (business_id) REFERENCES business(business_id)\n",
    ");\n",
    "\"\"\"\n",
    "create_review_table = \"\"\"\n",
    "CREATE TABLE review (\n",
    "    review_id VARCHAR(22) PRIMARY KEY,\n",
    "    business_id VARCHAR(22),\n",
    "    user_id VARCHAR(22),\n",
    "    stars INT,\n",
    "    date DATETIME,\n",
    "    text TEXT,\n",
    "    useful INT,\n",
    "    funny INT,\n",
    "    cool INT,\n",
    "    FOREIGN KEY (business_id) REFERENCES business(business_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_checkin_table = \"\"\"\n",
    "CREATE TABLE checkin (\n",
    "    business_id VARCHAR(22) PRIMARY KEY,\n",
    "    date TEXT,\n",
    "    FOREIGN KEY (business_id) REFERENCES business(business_id)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Information and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected.\n"
     ]
    }
   ],
   "source": [
    "server = 'group15-server.database.windows.net'  # Replace with your server name\n",
    "database = 'group15_Yelp_Database'  # Replace with your database name\n",
    "username = 'group15'  # Replace with your username\n",
    "password = 'Badam15123'  # Replace with your password\n",
    "driver = '{ODBC Driver 18 for SQL Server}'  # Ensure you have this driver installed\n",
    "conn = pyodbc.connect(\n",
    "    f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password};'\n",
    "    'Encrypt=yes;TrustServerCertificate=no;Connection Timeout=60;'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "print(\"Connected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widening columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "('String data, right truncation: length 602 buffer 510', 'HY000')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m checkin_params = [\u001b[33m\"\u001b[39m\u001b[33mcheckin_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mbusiness_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# or [\"business_id\",\"date\"] if IDENTITY\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Upload full DataFrames you already built\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mupload_with_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_flattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_insert_sql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m upload_with_query(conn, attributes_df, attributes_insert_sql, attributes_params, batch_size=\u001b[32m10000\u001b[39m)\n\u001b[32m     39\u001b[39m upload_with_query(conn, hours_df, hours_insert_sql, hours_params, batch_size=\u001b[32m10000\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mupload_with_query\u001b[39m\u001b[34m(conn, df, insert_sql, param_order, batch_size)\u001b[39m\n\u001b[32m      7\u001b[39m part = part.where(pd.notna(part), \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# NaN -> None\u001b[39;00m\n\u001b[32m      8\u001b[39m values = part.values.tolist()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_sql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m conn.commit()\n",
      "\u001b[31mProgrammingError\u001b[39m: ('String data, right truncation: length 602 buffer 510', 'HY000')"
     ]
    }
   ],
   "source": [
    "\n",
    "def upload_with_query(conn, df, insert_sql, param_order, batch_size=10000):\n",
    "    cur = conn.cursor()\n",
    "    cur.fast_executemany = True\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        part = df.iloc[start:start + batch_size]\n",
    "        part = part[param_order].astype(object)\n",
    "        part = part.where(pd.notna(part), None)  # NaN -> None\n",
    "        values = part.values.tolist()\n",
    "        cur.executemany(insert_sql, values)\n",
    "        conn.commit()\n",
    "\n",
    "# Example column lists â€“ adjust to match your actual CREATE TABLEs\n",
    "business_insert_sql = \"\"\"\n",
    "INSERT INTO business (\n",
    "  business_id, name, address, city, state, postal_code,\n",
    "  latitude, longitude, stars, review_count, is_open, categories\n",
    ") VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "attributes_insert_sql = \"INSERT INTO attributes (business_id, attribute_key, attribute_value) VALUES (?,?,?)\"\n",
    "hours_insert_sql = \"INSERT INTO hours (business_id, day, open_time, close_time) VALUES (?,?,?,?)\"\n",
    "review_insert_sql = \"\"\"\n",
    "  INSERT INTO review (\n",
    "  review_id, business_id, user_id, stars, date, text, useful, funny, cool\n",
    ") VALUES (?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "checkin_insert_sql = \"INSERT INTO checkin (business_id, date) VALUES (?,?)\"\n",
    "\n",
    "# Param orders (column order for binding)\n",
    "business_params = [\"business_id\",\"name\",\"address\",\"city\",\"state\",\"postal_code\",\n",
    "                   \"latitude\",\"longitude\",\"stars\",\"review_count\",\"is_open\",\"categories\"]\n",
    "attributes_params = [\"business_id\",\"attribute_key\",\"attribute_value\"]\n",
    "hours_params = [\"business_id\",\"day\",\"open_time\",\"close_time\"]\n",
    "review_params = [\"review_id\",\"business_id\",\"user_id\",\"stars\",\"date\",\"text\",\"useful\",\"funny\",\"cool\"]\n",
    "checkin_params = [\"checkin_id\",\"business_id\",\"date\"]  # or [\"business_id\",\"date\"] if IDENTITY\n",
    "\n",
    "# Upload full DataFrames you already built\n",
    "upload_with_query(conn, business_flattened, business_insert_sql, business_params, batch_size=10000)\n",
    "upload_with_query(conn, attributes_df, attributes_insert_sql, attributes_params, batch_size=10000)\n",
    "upload_with_query(conn, hours_df, hours_insert_sql, hours_params, batch_size=10000)\n",
    "upload_with_query(conn, reviews_df, review_insert_sql, review_params, batch_size=5000)\n",
    "upload_with_query(conn, checkin_df, checkin_insert_sql, checkin_params, batch_size=10000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
