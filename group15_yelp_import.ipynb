{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "import kagglehub\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "yelp_dataset = kagglehub.dataset_download('yelp-dataset/yelp-dataset')\n",
    "\n",
    "#load business data\n",
    "business_data = pd.read_json(f\"{yelp_dataset}/yelp_academic_dataset_business.json\", lines=True)\n",
    "\n",
    "#load reviews data in chunks\n",
    "reviews_chunks = pd.read_json(f\"{yelp_dataset}/yelp_academic_dataset_review.json\", lines=True, chunksize=10000) \n",
    "\n",
    "#store chunks in list of chunks\n",
    "reviews_df_list = []\n",
    "for chunk in reviews_chunks:\n",
    "    reviews_df_list.append(chunk)\n",
    "\n",
    "#combine to one df\n",
    "reviews_df = pd.concat(reviews_df_list, ignore_index=True)\n",
    "\n",
    "#load checkin data\n",
    "checkin_chunks = pd.read_json(f\"{yelp_dataset}/yelp_academic_dataset_checkin.json\", lines=True, chunksize=10000)\n",
    "checkin_df_list = []\n",
    "\n",
    "for chunk in checkin_chunks:\n",
    "    checkin_df_list.append(chunk)\n",
    "#combine to one df\n",
    "checkin_df = pd.concat(checkin_df_list, ignore_index=True)\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening Business into 3 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_attributes(row):\n",
    "    attributes = row.get('attributes', {})\n",
    "    if attributes:\n",
    "        attributes = json.loads(attributes) if isinstance(attributes, str) else attributes\n",
    "        flattened = []\n",
    "        for key, value in attributes.items():\n",
    "            if key == 'RestaurantsPriceRange2':\n",
    "                key = 'PriceRange'\n",
    "            elif key == 'RestaurantsDelivery':\n",
    "                key = 'Delivery'\n",
    "            elif key == 'RestaurantsTakeOut':\n",
    "                key = 'Takeout'\n",
    "            flattened.append({'business_id': row['business_id'], 'attribute_key': key, 'attribute_value': value})\n",
    "        return flattened\n",
    "    return []\n",
    "\n",
    "def flatten_hours(row):\n",
    "    hours = row.get('hours', {})\n",
    "    if hours:\n",
    "        hours = json.loads(hours) if isinstance(hours, str) else hours\n",
    "        flattened = []\n",
    "        for day, time_range in hours.items():\n",
    "            open_time, close_time = time_range.split('-')\n",
    "            flattened.append({'business_id': row['business_id'], 'day': day, 'open_time': open_time, 'close_time': close_time})\n",
    "        return flattened\n",
    "    return []\n",
    "\n",
    "# Flatten attributes and hours\n",
    "attributes_data = []\n",
    "hours_data = []\n",
    "\n",
    "for _, row in business_data.iterrows():\n",
    "    attributes_data.extend(flatten_attributes(row))\n",
    "    hours_data.extend(flatten_hours(row))\n",
    "\n",
    "# Convert to DataFrames\n",
    "attributes_df = pd.DataFrame(attributes_data)\n",
    "hours_df = pd.DataFrame(hours_data)\n",
    "\n",
    "# Drop the original nested fields from the business table\n",
    "business_flattened = business_data.drop(columns=['attributes', 'hours'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tables Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_business_table = \"\"\"\n",
    "IF OBJECT_ID('dbo.business','U') IS NULL\n",
    "CREATE TABLE business (\n",
    "    business_id VARCHAR(22) PRIMARY KEY,\n",
    "    name VARCHAR(255),\n",
    "    address VARCHAR(255),\n",
    "    city VARCHAR(100),\n",
    "    state VARCHAR(10),\n",
    "    postal_code VARCHAR(20),\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    stars FLOAT,\n",
    "    review_count INT,\n",
    "    is_open BIT,\n",
    "    categories TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_attributes_table = \"\"\"\n",
    "IF OBJECT_ID('dbo.attributes','U') IS NULL\n",
    "CREATE TABLE attributes (\n",
    "    attribute_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    business_id VARCHAR(22),\n",
    "    attribute_key VARCHAR(100),\n",
    "    attribute_value VARCHAR(MAX),\n",
    "    FOREIGN KEY (business_id) REFERENCES business(business_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_hours_table = \"\"\"\n",
    "IF OBJECT_ID('dbo.hours','U') IS NULL\n",
    "CREATE TABLE hours (\n",
    "    hour_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    business_id VARCHAR(22),\n",
    "    day VARCHAR(20),\n",
    "    open_time VARCHAR(20),\n",
    "    close_time VARCHAR(20),\n",
    "    FOREIGN KEY (business_id) REFERENCES business(business_id)\n",
    ");\n",
    "\"\"\"\n",
    "create_review_table = \"\"\"\n",
    "IF OBJECT_ID('dbo.review','U') IS NULL\n",
    "CREATE TABLE review (\n",
    "    review_id VARCHAR(22) PRIMARY KEY,\n",
    "    business_id VARCHAR(22),\n",
    "    user_id VARCHAR(22),\n",
    "    stars INT,\n",
    "    date DATETIME,\n",
    "    text TEXT,\n",
    "    useful INT,\n",
    "    funny INT,\n",
    "    cool INT,\n",
    "    FOREIGN KEY (business_id) REFERENCES business(business_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_checkin_table = \"\"\"\n",
    "IF OBJECT_ID('dbo.checkin','U') IS NULL\n",
    "CREATE TABLE checkin (\n",
    "    business_id VARCHAR(22) PRIMARY KEY,\n",
    "    date TEXT,\n",
    "    FOREIGN KEY (business_id) REFERENCES business(business_id)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Information and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected.\n"
     ]
    }
   ],
   "source": [
    "#connects to sql server\n",
    "server = 'group15-server.database.windows.net'  \n",
    "database = 'group15_Yelp_Database'  \n",
    "username = 'group15'  \n",
    "password = 'Badam15123'  \n",
    "driver = '{ODBC Driver 18 for SQL Server}' \n",
    "conn = pyodbc.connect(\n",
    "    f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password};'\n",
    "    'Encrypt=yes;TrustServerCertificate=no;Connection Timeout=60;'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "print(\"Connected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates tables if not done already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created (or already existed).\n"
     ]
    }
   ],
   "source": [
    "#creates tables using queries above\n",
    "cur = conn.cursor()\n",
    "for sql in [\n",
    "    create_business_table,\n",
    "    create_attributes_table,\n",
    "    create_hours_table,\n",
    "    create_review_table,\n",
    "    create_checkin_table,\n",
    "]:\n",
    "    cur.execute(sql)\n",
    "conn.commit()\n",
    "print(\"Tables created (or already existed).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widening columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDL applied.\n"
     ]
    }
   ],
   "source": [
    "#makes the table attributes large enough to contain the large text required\n",
    "cur = conn.cursor()\n",
    "ddl = [\n",
    "    \"ALTER TABLE business   ALTER COLUMN categories NVARCHAR(MAX) NULL;\",\n",
    "    \"ALTER TABLE review     ALTER COLUMN text       NVARCHAR(MAX) NULL;\",\n",
    "    \"ALTER TABLE checkin    ALTER COLUMN date       NVARCHAR(MAX) NULL;\",\n",
    "    \"ALTER TABLE attributes ALTER COLUMN attribute_value NVARCHAR(MAX) NULL;\",\n",
    "]\n",
    "for s in ddl:\n",
    "    try:\n",
    "        cur.execute(s)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(s, \"->\", e)\n",
    "print(\"DDL applied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philadelphia, PA businesses: 14,575\n",
      "Philadelphia, PA attributes: 127,463\n",
      "Philadelphia, PA hours: 74,168\n",
      "Philadelphia, PA checkins: 12,899\n",
      "Philadelphia, PA reviews: 967,871\n"
     ]
    }
   ],
   "source": [
    "# Cell: Philadelphia, PA filtered DataFrames (robust)\n",
    "\n",
    "# Choose the business source (flattened if available)\n",
    "business_src = business_flattened if 'business_flattened' in globals() else business_data\n",
    "\n",
    "bd = business_src.copy()\n",
    "bd[\"city_norm\"] = bd[\"city\"].astype(str).str.strip()\n",
    "bd[\"state_norm\"] = bd[\"state\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Strict Philadelphia, PA filter\n",
    "philadelphia_business_df = bd[\n",
    "    (bd[\"city_norm\"].str.lower() == \"philadelphia\") &\n",
    "    (bd[\"state_norm\"] == \"PA\")\n",
    "].drop(columns=[\"city_norm\",\"state_norm\"], errors=\"ignore\")\n",
    "\n",
    "# If nothing found, print quick diagnostics\n",
    "if len(philadelphia_business_df) == 0:\n",
    "    print(\"No Philadelphia, PA businesses found. Diagnostics:\")\n",
    "    print(\"Unique states (sample):\", bd[\"state_norm\"].dropna().unique()[:10])\n",
    "    print(\"Top cities in PA:\")\n",
    "    print(bd[bd[\"state_norm\"] == \"PA\"][\"city_norm\"].value_counts().head(20))\n",
    "\n",
    "philadelphia_bids = set(philadelphia_business_df[\"business_id\"])\n",
    "\n",
    "# Related tables filtered by business_id\n",
    "philadelphia_attributes_df = attributes_df[attributes_df[\"business_id\"].isin(philadelphia_bids)].copy()\n",
    "philadelphia_hours_df = hours_df[hours_df[\"business_id\"].isin(philadelphia_bids)].copy()\n",
    "philadelphia_checkin_df = checkin_df[checkin_df[\"business_id\"].isin(philadelphia_bids)].copy()\n",
    "\n",
    "# Filter reviews by Philadelphia business_ids\n",
    "philadelphia_reviews_df = reviews_df[reviews_df[\"business_id\"].isin(philadelphia_bids)].copy()\n",
    "\n",
    "# De-duplicate on natural keys\n",
    "if len(philadelphia_attributes_df):\n",
    "    philadelphia_attributes_df = (\n",
    "        philadelphia_attributes_df.sort_values([\"business_id\",\"attribute_key\"])\n",
    "        .drop_duplicates([\"business_id\",\"attribute_key\"], keep=\"last\")\n",
    "    )\n",
    "if len(philadelphia_hours_df):\n",
    "    philadelphia_hours_df = (\n",
    "        philadelphia_hours_df.sort_values([\"business_id\",\"day\"])\n",
    "        .drop_duplicates([\"business_id\",\"day\"], keep=\"last\")\n",
    "    )\n",
    "\n",
    "print(f\"Philadelphia, PA businesses: {len(philadelphia_business_df):,}\")\n",
    "print(f\"Philadelphia, PA attributes: {len(philadelphia_attributes_df):,}\")\n",
    "print(f\"Philadelphia, PA hours: {len(philadelphia_hours_df):,}\")\n",
    "print(f\"Philadelphia, PA checkins: {len(philadelphia_checkin_df):,}\")\n",
    "print(f\"Philadelphia, PA reviews: {len(philadelphia_reviews_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable batch size for Azure SQL (recommend 500â€“1000 on free tier)\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "def truncate_review_table(conn, retries=3):\n",
    "    \"\"\"\n",
    "    Empties the review table before uploading with retry on connection failure.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"TRUNCATE TABLE review;\")\n",
    "            conn.commit()\n",
    "            print(\"Truncated table: review\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} to truncate review failed with error: {e}\")\n",
    "            conn.close()\n",
    "            # Recreate connection (assuming a function get_connection() exists)\n",
    "            conn = get_connection()\n",
    "            attempt += 1\n",
    "    raise RuntimeError(\"Failed to truncate review table after multiple retries.\")\n",
    "\n",
    "\n",
    "def upload_with_query(conn, df, insert_sql, param_order, batch_size=1000, table_name=\"\"):\n",
    "    \"\"\"\n",
    "    Generic batch uploader with progress logs.\n",
    "    Converts NaN to None and uses fast_executemany for speed.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        print(f\"[{table_name}] Nothing to upload.\")\n",
    "        return\n",
    "    cur = conn.cursor()\n",
    "    cur.fast_executemany = True\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        part = df.iloc[start:end]\n",
    "        part = part[param_order].astype(object)\n",
    "        part = part.where(pd.notna(part), None)  # NaN -> None\n",
    "        values = part.values.tolist()\n",
    "        cur.executemany(insert_sql, values)\n",
    "        conn.commit()\n",
    "        print(f\"[{table_name}] Uploaded {end}/{total} rows; remaining {total - end}.\")\n",
    "\n",
    "# MERGE/UPSERT for review on primary key review_id to avoid duplicates on retries\n",
    "merge_review_sql = \"\"\"\n",
    "MERGE review AS tgt\n",
    "USING (VALUES (?,?,?,?,?,?,?,?,?)) AS src (review_id,business_id,user_id,stars,[date],[text],useful,funny,cool)\n",
    "ON tgt.review_id = src.review_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    tgt.business_id = src.business_id,\n",
    "    tgt.user_id     = src.user_id,\n",
    "    tgt.stars       = src.stars,\n",
    "    tgt.[date]      = src.[date],\n",
    "    tgt.[text]      = src.[text],\n",
    "    tgt.useful      = src.useful,\n",
    "    tgt.funny       = src.funny,\n",
    "    tgt.cool        = src.cool\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (review_id,business_id,user_id,stars,[date],[text],useful,funny,cool)\n",
    "  VALUES (src.review_id,src.business_id,src.user_id,src.stars,src.[date],src.[text],src.useful,src.funny,src.cool);\n",
    "\"\"\"\n",
    "\n",
    "def upload_reviews_upsert(conn, df, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Batch UPSERT for reviews using MERGE to prevent duplicates.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        print(\"[review] Nothing to upload.\")\n",
    "        return\n",
    "    cur = conn.cursor()\n",
    "    cur.fast_executemany = True\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        part = df.iloc[start:end]\n",
    "        part = part[[\"review_id\",\"business_id\",\"user_id\",\"stars\",\"date\",\"text\",\"useful\",\"funny\",\"cool\"]].astype(object)\n",
    "        part = part.where(pd.notna(part), None)\n",
    "        values = part.values.tolist()\n",
    "        cur.executemany(merge_review_sql, values)  # executes per-row merge efficiently\n",
    "        conn.commit()\n",
    "        print(f\"[review] Upserted {end}/{total} rows; remaining {total - end}.\")\n",
    "\n",
    "def upload_business_upsert(conn, df, batch_size=BATCH_SIZE):\n",
    "    merge_sql = \"\"\"\n",
    "    MERGE business AS tgt\n",
    "    USING (VALUES (?,?,?,?,?,?,?,?,?,?,?,?)) AS src\n",
    "      (business_id,name,address,city,state,postal_code,latitude,longitude,stars,review_count,is_open,categories)\n",
    "    ON tgt.business_id = src.business_id\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      name = src.name,\n",
    "      address = src.address,\n",
    "      city = src.city,\n",
    "      state = src.state,\n",
    "      postal_code = src.postal_code,\n",
    "      latitude = src.latitude,\n",
    "      longitude = src.longitude,\n",
    "      stars = src.stars,\n",
    "      review_count = src.review_count,\n",
    "      is_open = src.is_open,\n",
    "      categories = src.categories\n",
    "    WHEN NOT MATCHED THEN\n",
    "      INSERT (business_id,name,address,city,state,postal_code,latitude,longitude,stars,review_count,is_open,categories)\n",
    "      VALUES (src.business_id,src.name,src.address,src.city,src.state,src.postal_code,src.latitude,src.longitude,src.stars,src.review_count,src.is_open,src.categories);\n",
    "    \"\"\"\n",
    "    cols = [\"business_id\",\"name\",\"address\",\"city\",\"state\",\"postal_code\",\"latitude\",\"longitude\",\"stars\",\"review_count\",\"is_open\",\"categories\"]\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        print(\"[business] Nothing to upsert.\")\n",
    "        return\n",
    "    cur = conn.cursor()\n",
    "    cur.fast_executemany = True\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        part_df = df.iloc[start:end][cols]\n",
    "        part = part_df.astype(object).where(pd.notna(part_df), None)\n",
    "        cur.executemany(merge_sql, part.values.tolist())\n",
    "        conn.commit()\n",
    "        print(f\"[business] Upserted {end}/{total}; remaining {total - end}.\")\n",
    "\n",
    "def upload_checkin_upsert(conn, df, batch_size=BATCH_SIZE):\n",
    "    merge_sql = \"\"\"\n",
    "    MERGE checkin AS tgt\n",
    "    USING (VALUES (?,?)) AS src (business_id,[date])\n",
    "    ON tgt.business_id = src.business_id\n",
    "    WHEN MATCHED THEN UPDATE SET [date] = src.[date]\n",
    "    WHEN NOT MATCHED THEN INSERT (business_id,[date]) VALUES (src.business_id,src.[date]);\n",
    "    \"\"\"\n",
    "    cols = [\"business_id\",\"date\"]\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        print(\"[checkin] Nothing to upsert.\")\n",
    "        return\n",
    "    cur = conn.cursor()\n",
    "    cur.fast_executemany = True\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        part_df = df.iloc[start:end][cols]\n",
    "        part = part_df.astype(object).where(pd.notna(part_df), None)\n",
    "        cur.executemany(merge_sql, part.values.tolist())\n",
    "        conn.commit()\n",
    "        print(f\"[checkin] Upserted {end}/{total}; remaining {total - end}.\")\n",
    "\n",
    "def delete_rows_for_business_ids(conn, table, business_ids, batch_size=BATCH_SIZE):\n",
    "    assert table in {\"attributes\", \"hours\"}, \"Only attributes/hours deletion is allowed.\"\n",
    "    ids = list(set(business_ids))\n",
    "    total = len(ids)\n",
    "    if total == 0:\n",
    "        print(f\"[{table}] No rows to delete.\")\n",
    "        return\n",
    "    cur = conn.cursor()\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        chunk = ids[start:end]\n",
    "        cur.executemany(f\"DELETE FROM {table} WHERE business_id = ?\", [(x,) for x in chunk])\n",
    "        conn.commit()\n",
    "        print(f\"[{table}] Deleted for {end}/{total} business_ids; remaining {total - end}.\")\n",
    "\n",
    "business_insert_sql = \"\"\"\n",
    "INSERT INTO business (\n",
    "  business_id, name, address, city, state, postal_code,\n",
    "  latitude, longitude, stars, review_count, is_open, categories\n",
    ") VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "attributes_insert_sql = \"INSERT INTO attributes (business_id, attribute_key, attribute_value) VALUES (?,?,?)\"\n",
    "hours_insert_sql = \"INSERT INTO hours (business_id, day, open_time, close_time) VALUES (?,?,?,?)\"\n",
    "\n",
    "# Bracket [date] and [text]\n",
    "review_insert_sql = \"\"\"\n",
    "INSERT INTO review (\n",
    "  review_id, business_id, user_id, stars, [date], [text], useful, funny, cool\n",
    ") VALUES (?,?,?,?,?,?,?,?,?)\n",
    "\"\"\"\n",
    "\n",
    "# use business id as primary key\n",
    "checkin_insert_sql = \"INSERT INTO checkin (business_id, [date]) VALUES (?,?)\"\n",
    "checkin_params = [\"business_id\",\"date\"]\n",
    "\n",
    "# Param orders (column order for binding)\n",
    "business_params = [\"business_id\",\"name\",\"address\",\"city\",\"state\",\"postal_code\",\n",
    "                   \"latitude\",\"longitude\",\"stars\",\"review_count\",\"is_open\",\"categories\"]\n",
    "attributes_params = [\"business_id\",\"attribute_key\",\"attribute_value\"]\n",
    "hours_params = [\"business_id\",\"day\",\"open_time\",\"close_time\"]\n",
    "review_params = [\"review_id\",\"business_id\",\"user_id\",\"stars\",\"date\",\"text\",\"useful\",\"funny\",\"cool\"]\n",
    "checkin_params = [\"business_id\",\"date\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "('08S01', '[08S01] [Microsoft][ODBC Driver 18 for SQL Server]Communication link failure (0) (SQLExecDirectW)')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Truncate review table before upload\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtruncate_review_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Upload (upsert) business and related filtered data\u001b[39;00m\n\u001b[32m      5\u001b[39m upload_business_upsert(conn, philadelphia_business_df, batch_size=BATCH_SIZE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtruncate_review_table\u001b[39m\u001b[34m(conn)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mEmpties the review table before uploading (faster than DELETE).\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03mSafe because no other table references review via FK.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m cur = conn.cursor()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTRUNCATE TABLE review;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m conn.commit()\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTruncated table: review\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOperationalError\u001b[39m: ('08S01', '[08S01] [Microsoft][ODBC Driver 18 for SQL Server]Communication link failure (0) (SQLExecDirectW)')"
     ]
    }
   ],
   "source": [
    "\n",
    "# Truncate review table before upload\n",
    "truncate_review_table(conn)\n",
    "\n",
    "# Upload (upsert) business and related filtered data\n",
    "upload_business_upsert(conn, philadelphia_business_df, batch_size=BATCH_SIZE)\n",
    "upload_checkin_upsert(conn, philadelphia_checkin_df, batch_size=BATCH_SIZE)\n",
    "upload_with_query(conn, philadelphia_attributes_df, attributes_insert_sql, attributes_params, batch_size=BATCH_SIZE, table_name=\"attributes\")\n",
    "upload_with_query(conn, philadelphia_hours_df, hours_insert_sql, hours_params, batch_size=BATCH_SIZE, table_name=\"hours\")\n",
    "upload_reviews_upsert(conn, philadelphia_reviews_df, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Print sizes for verification\n",
    "print(f\"Philadelphia, PA businesses: {len(philadelphia_business_df):,}\")\n",
    "print(f\"Philadelphia, PA attributes: {len(philadelphia_attributes_df):,}\")\n",
    "print(f\"Philadelphia, PA hours: {len(philadelphia_hours_df):,}\")\n",
    "print(f\"Philadelphia, PA checkins: {len(philadelphia_checkin_df):,}\")\n",
    "print(f\"Philadelphia, PA reviews: {len(philadelphia_reviews_df):,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
